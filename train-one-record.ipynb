{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    # FYI export HF_TOKEN=$(pbpaste)\n",
    "    raise ValueError(\"You need to set HF_TOKEN environment variable. Get one from huggingface.co\")\n",
    "\n",
    "# Load the shell/bash subset\n",
    "#   FYI:   https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/shell (FYI gated, must give email and approve)\n",
    "full_dataset = load_dataset( \"bigcode/the-stack\", split=\"train\", data_dir=\"data/shell\")  # , lang=[\"bash\"])\n",
    "# data_dir data/shell has 11 files, not bad size (about 4GB IIEC)\n",
    "# print(full_dataset.shape)\n",
    "# print(f\"  full: size(bytes) comma delimited: {full_dataset.size_in_bytes/1024/1024:,.0f} MB\")\n",
    "\n",
    "first_x = full_dataset.take(35000)\n",
    "# print(first_x.shape)\n",
    "# print(f\"  first-{len(first_x)}: size(bytes) comma delimited: {first_x.data.nbytes/1024/1024:,.4f} MB\")\n",
    "\n",
    "# Save locally in diff format if needed, I am going to stick with parquet\n",
    "# full_dataset.to_csv(\"shell_scripts.csv\")\n",
    "# print(full_dataset.column_names)\n",
    "subset = full_dataset.select(range(1))\n",
    "print(\"shape\", subset.shape)\n",
    "\n",
    "original_columns = subset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus\n",
    "scenario_path = \"tmp/one-record/original\"\n",
    "if not os.path.exists(scenario_path):\n",
    "    os.makedirs(scenario_path, exist_ok=True)\n",
    "\n",
    "corpus_file = f\"{scenario_path}/corpus\"\n",
    "if not os.path.exists(corpus_file):\n",
    "    with open(corpus_file, \"w\") as f:\n",
    "        for example in subset[\"content\"]:  # Adjust \"content\" to match your dataset key\n",
    "            f.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer_path = f\"{scenario_path}/tokenizer\"\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    os.makedirs(tokenizer_path)\n",
    "\n",
    "# Train the tokenizer\n",
    "if not os.path.exists(tokenizer_path + \"/vocab.json\"):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[corpus_file], vocab_size=8000, min_frequency=2)  # PRN adjust vocab_size/min_frequency?\n",
    "    tokenizer.save_model(tokenizer_path)\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(tokenizer_path + \"/vocab.json\", tokenizer_path + \"/merges.txt\")\n",
    "\n",
    "# tokenize (FYI .map is immutable, a new dataset is returned with the added column)\n",
    "print(\"before tokenize dataset size: \", subset.shape)\n",
    "subset = subset.map(lambda x: {\"tokens\": tokenizer.encode(x[\"content\"]).ids})\n",
    "print(\"after tokenize dataset size: \", subset.shape)\n",
    "print()\n",
    "print(subset[\"tokens\"][0])  # Example tokenized output\n",
    "print()\n",
    "#\n",
    "# # VIEW SOME TOKENS:\n",
    "# # show each token for first 10:\n",
    "# for i in range(100):\n",
    "#     print(subset_tokenizd[\"tokens\"][0][i], tokenizer.decode([subset_tokenizd[\"tokens\"][0][i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(tokens, seq_len):\n",
    "    # Split into sequences of length seq_len + 1\n",
    "    sequences = [tokens[i:i + seq_len + 1] for i in range(len(tokens) - seq_len)]\n",
    "    return sequences\n",
    "\n",
    "seq_len = 50\n",
    "subset = subset.map(lambda x: {\"sequences\": create_training_pairs(x[\"tokens\"], seq_len)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECTING SUBSET with new columns:\n",
    "def print_subset():\n",
    "    print(subset.shape)\n",
    "    for name in subset.column_names:\n",
    "        print(f\"{name}: {subset[name]}\")\n",
    "        print(f\"  {len(subset[name])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_subset_new_columns():\n",
    "    for name in subset.column_names:\n",
    "        if name in original_columns:\n",
    "            continue\n",
    "        print(f\"{name}: {subset[name]}\")\n",
    "        first_record = subset[name][0]\n",
    "        if isinstance(first_record, list):\n",
    "            print(f\"  {first_record}\")\n",
    "            print(f\"  {len(first_record)} records\")\n",
    "        # if list has lists:\n",
    "        first_element_of_first_record = first_record[0]\n",
    "        if isinstance(first_element_of_first_record, list):\n",
    "            print(f\"    {first_element_of_first_record}\")\n",
    "            print(f\"    {len(first_element_of_first_record)} items\")\n",
    "        \n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "print_subset()\n",
    "\n",
    "print()\n",
    "print(\"# SEQUENCES:\")\n",
    "print()\n",
    "for i in range(3):\n",
    "    # each record (in original dataset) now has a list of sequences (so, sequences is not a scalar value like content)\n",
    "    print(\"## sequence \" + str(i))\n",
    "    print(subset[\"sequences\"][0][i])\n",
    "    print(tokenizer.decode(subset[\"sequences\"][0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input and target\n",
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "# wrong:\n",
    "# def mapper(x):\n",
    "#     what = x['sequences']\n",
    "#     split = split_input_target(what)\n",
    "#     print(f\"* {len(what)} -> {len(split[0])} {len(split[1])}\")\n",
    "#     return {\"x_input\": split[0], \"y_target\": split[1]}\n",
    "\n",
    "def mapper(all):\n",
    "    sequences = all[\"sequences\"]\n",
    "    splits = [split_input_target(sequence) for sequence in sequences]\n",
    "    x_input = [split[0] for split in splits]\n",
    "    y_target =  [split[1] for split in splits]\n",
    "    return {\"x_input\": x_input, \"y_target\": y_target}\n",
    "subset = subset.map(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_subset_new_columns()\n",
    "a = [1,2,3,4]\n",
    "print(a[:-1])\n",
    "print(a[1:])\n",
    "print(split_input_target(a))\n",
    "\n",
    "print_subset_new_columns()\n",
    "\n",
    "print(subset[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CodeCompletionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer(x, x)  # Encoder-only transformer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "vocab_size = 8000\n",
    "# mps_device = torch.device(\"mps\")\n",
    "model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "# model.to(mps_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ALTERNATE (less complexity, should train faster and maybe be useful)\n",
    "class AlternateCodeCompletionLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "# model = AlternateCodeCompletionLSTM(vocab_size=8000, embed_size=128, hidden_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3428381085395813\n",
      "Epoch 2, Loss: 0.2325245440006256\n",
      "Epoch 3, Loss: 0.18840189278125763\n",
      "Epoch 4, Loss: 0.23613181710243225\n",
      "Epoch 5, Loss: 0.15484541654586792\n",
      "Epoch 6, Loss: 0.12867139279842377\n",
      "Epoch 7, Loss: 0.10232086479663849\n",
      "Epoch 8, Loss: 0.12792468070983887\n",
      "Epoch 9, Loss: 0.08423150330781937\n",
      "Epoch 10, Loss: 0.09120484441518784\n",
      "Epoch 11, Loss: 0.10631529241800308\n",
      "Epoch 12, Loss: 0.05978334695100784\n",
      "Epoch 13, Loss: 0.07507678866386414\n",
      "Epoch 14, Loss: 0.04323822632431984\n",
      "Epoch 15, Loss: 0.08547110110521317\n",
      "Epoch 16, Loss: 0.07593853026628494\n",
      "Epoch 17, Loss: 0.08188261836767197\n",
      "Epoch 18, Loss: 0.032942067831754684\n",
      "Epoch 19, Loss: 0.05927233025431633\n",
      "Epoch 20, Loss: 0.061810266226530075\n",
      "Epoch 21, Loss: 0.029853317886590958\n",
      "Epoch 22, Loss: 0.02651490829885006\n",
      "Epoch 23, Loss: 0.026656942442059517\n",
      "Epoch 24, Loss: 0.024634717032313347\n",
      "Epoch 25, Loss: 0.04473059996962547\n",
      "Epoch 26, Loss: 0.014352821744978428\n",
      "Epoch 27, Loss: 0.02192622423171997\n",
      "Epoch 28, Loss: 0.0093899080529809\n",
      "Epoch 29, Loss: 0.005041217897087336\n",
      "Epoch 30, Loss: 0.02729143388569355\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DataLoader\n",
    "class CodeDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "# Prepare data\n",
    "# inputs = subset[\"x_input\"]\n",
    "# targets = subset[\"y_target\"]\n",
    "#inputs = subset[\"x_input\"][0]\n",
    "#targets = subset[\"y_target\"][0]\n",
    "inputs = [ item for sublist in subset[\"x_input\"] for item in sublist]\n",
    "targets = [ item for sublist in subset[\"y_target\"] for item in sublist]\n",
    "train_loader = DataLoader(CodeDataset(inputs, targets), batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):  # Adjust epochs as needed\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "# 40 epochs on first record sequences only... got to 11/17% loss, IIAC because data set is small, its easy to overfit it?\n",
    "# 50 epochs => 0.029 down to 0.005 loss... and it gives reasonable completions for the one example, well formatted shell script code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model:\n",
    "torch.save(model.state_dict(), f\"{scenario_path}/checkpoint001-50epochs.pth\")\n",
    "\n",
    "def load_model():\n",
    "    model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "    # Load the saved state_dict into the model\n",
    "    model.load_state_dict(torch.load(f\"{scenario_path}/checkpoint001.pth\"))\n",
    "    #model.to(mps_device)\n",
    "\n",
    "# load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [365, 28, 6, 317, 220, 68, 87, 68, 66, 220, 431, 220, 14, 431, 404, 66, 83, 75, 6, 198, 198, 2, 313, 62, 314, 296, 340, 1, 198, 313, 62, 314, 296, 285, 1, 198, 2, 220, 271, 62, 364, 220, 392, 68, 383, 415, 403, 68, 220, 79]\n",
      "## all:\n",
      "\u001b[32mKUBECTL='docker exec hyperkube /hyperkube kubectl'\n",
      "\n",
      "#RUN_SKYDNS=\"yes\"\n",
      "RUN_SKYDNS=\"no\"\n",
      "# DNS_ARGUMENTS needs to be p\u001b[0m\u001b[31massed passed Kubernetes is setup.\n",
      "if [ \"${RUN_SKYDNS}\" = \"yes\" ]; then\n",
      "\tDNS_ARGUMENTS=\"--cluster-dns=10.0.0.10 --\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_code(model, tokenizer, prompt, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).ids  # Convert prompt to tokens\n",
    "    if len(tokens) < max_len:\n",
    "        raise Exception(\"prompt too short\")\n",
    "        # pad didn't \"work\" but I only tested it one time so yeah... and on a miniscule dataset :)\n",
    "        padding = tokenizer.encode(\"\\n\" * (max_len - len(tokens))).ids\n",
    "        padding.extend(tokens)\n",
    "        tokens = padding\n",
    "\n",
    "    print(\"tokens:\", tokens)\n",
    "    generated_tokens = []\n",
    "    tokens = tokens[-max_len:]  # truncate to max_len\n",
    "    original_decoded = tokenizer.decode(tokens)\n",
    "    for _ in range(max_len):\n",
    "        last_max_len_tokens = tokens[-max_len:]\n",
    "        input_tensor = torch.tensor(last_max_len_tokens).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(input_tensor)  #.contiguous().to(mps_device))  # Predict next token\n",
    "        next_token = output[0, -1].argmax(-1).item()  # Get the highest probability token\n",
    "        tokens.append(next_token)  # Add the next token\n",
    "        generated_tokens.append(next_token)\n",
    "        if next_token == tokenizer.token_to_id(\"<END>\"):  # Stop at <END> token\n",
    "            break\n",
    "    return original_decoded, tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "# TODO padding issue, NEED TO FIX in tokenizer? and? retrain? or? just pad bogus chars?\n",
    "# print(generate_code(model, tokenizer, \"#!/bin/bash\\n\"))\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\nfi\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\techo 1\\n\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuckfaceretarted\"  # too short\n",
    "# wow in the case of me rambling swear words, it put more gibberish after it like random words too! cool... versus an `fi` to close out\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuck'\\n\"  # HERE b/c it was closed it started to add diff logic below.. wrong but interesting to generate what was complete logic blocks (partially correct just not across mutliple lines but who cares... it even got indentation and changed out what was happening in an if block!)\n",
    "test_prompt=\"KUBECTL='docker exec hyperkube /hyperkube kubectl'\\n\\n#RUN_SKYDNS=\\\"yes\\\"\\nRUN_SKYDNS=\\\"no\\\"\\n# DNS_ARGUMENTS needs to be p\"\n",
    "# if [ \"${RUN_SKYDNS}\" = \"yes\" ]; then\n",
    "# \tDNS_ARGUMENTS=\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\"\n",
    "# else\n",
    "# \tDNS_ARGUMENTS=\"\"\n",
    "# fi\n",
    "# \"\n",
    "from colorama import Fore, Style\n",
    "\n",
    "original, generated = generate_code(model, tokenizer, test_prompt)\n",
    "print(\"## all:\")\n",
    "print(f\"{Fore.GREEN}{original}{Style.RESET_ALL}{Fore.RED}{generated}{Style.RESET_ALL}\")\n",
    "print()\n",
    "# TODO strip new lines? or can I collapse down to one with a count?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
