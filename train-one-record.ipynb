{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    # FYI export HF_TOKEN=$(pbpaste)\n",
    "    raise ValueError(\"You need to set HF_TOKEN environment variable. Get one from huggingface.co\")\n",
    "\n",
    "# Load the shell/bash subset\n",
    "#   FYI:   https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/shell (FYI gated, must give email and approve)\n",
    "full_dataset = load_dataset( \"bigcode/the-stack\", split=\"train\", data_dir=\"data/shell\")  # , lang=[\"bash\"])\n",
    "# data_dir data/shell has 11 files, not bad size (about 4GB IIEC)\n",
    "# print(full_dataset.shape)\n",
    "# print(f\"  full: size(bytes) comma delimited: {full_dataset.size_in_bytes/1024/1024:,.0f} MB\")\n",
    "\n",
    "first_x = full_dataset.take(35000)\n",
    "# print(first_x.shape)\n",
    "# print(f\"  first-{len(first_x)}: size(bytes) comma delimited: {first_x.data.nbytes/1024/1024:,.4f} MB\")\n",
    "\n",
    "# Save locally in diff format if needed, I am going to stick with parquet\n",
    "# full_dataset.to_csv(\"shell_scripts.csv\")\n",
    "# print(full_dataset.column_names)\n",
    "subset = full_dataset.select(range(100))\n",
    "print(\"shape\", subset.shape)\n",
    "\n",
    "original_columns = subset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())  # Should return True\n",
    "mps_device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = subset\n",
    "scenario_path = f\"tmp/one-record/full100\"\n",
    "\n",
    "# *** build tokenizer model ***\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer_path = f\"{scenario_path}/tokenizer\"\n",
    "os.makedirs(tokenizer_path, exist_ok=True)\n",
    "\n",
    "vocab_path = f\"{tokenizer_path}/vocab.json\"\n",
    "vocab_size = 8000\n",
    "merges_path = f\"{tokenizer_path}/merges.txt\"\n",
    "\n",
    "if not os.path.exists(vocab_path):\n",
    "    print(\"building tokenizer... (did not find saved model)\")\n",
    "    # *** CORPUS ***\n",
    "    # PRN would it be advantageous to use larger set of records for the corpus than I am going to train on?\n",
    "    corpus_file = f\"{scenario_path}/corpus\"\n",
    "    if not os.path.exists(corpus_file):\n",
    "        print(\"building corpus... (did not find saved file)\")\n",
    "        with open(corpus_file, \"w\") as f:\n",
    "            for example in corpus_dataset[\"content\"]:  # Adjust \"content\" to match your dataset key\n",
    "                f.write(example + \"\\n\")\n",
    "    #13.5s for FULL dataset! only have to do this once!\n",
    "\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[corpus_file], vocab_size=vocab_size, min_frequency=2)  # PRN adjust vocab_size/min_frequency?\n",
    "    tokenizer.save_model(tokenizer_path)\n",
    "\n",
    "    os.remove(corpus_file)  # huge file, easy to rebuild, so just nuke it after training\n",
    "\n",
    "\n",
    "def load_saved_tokenizer():\n",
    "    new_tokenizer = ByteLevelBPETokenizer(vocab_path, merges_path)\n",
    "    new_tokenizer.add_tokens(['<PAD>', \"<END>\"])\n",
    "\n",
    "    # apparently special tokens are not saved, WTF?, if I need to train on these they can't be different so throw if a problem:\n",
    "    if new_tokenizer.token_to_id('<PAD>') != 8000:\n",
    "        raise ValueError('Padding token not added correctly')\n",
    "    if new_tokenizer.token_to_id('<END>') != 8001:\n",
    "        raise ValueError('End token not added correctly')\n",
    "    global vocab_size\n",
    "    vocab_size = len(new_tokenizer.get_vocab())  # make sure reflects actual size\n",
    "    return new_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = load_saved_tokenizer()\n",
    "pad_token_id = tokenizer.token_to_id(\"<PAD>\")\n",
    "end_token_id = tokenizer.token_to_id(\"<END>\")\n",
    "# PRN add more special token as needed at end so I don't have to retrain tokenizer to add new special tokens (just obvi have to retrain the model or parts of it)\n",
    "\n",
    "# tokenize (FYI .map is immutable, a new dataset is returned with the added column)\n",
    "print(\"before tokenize dataset size: \", subset.shape)\n",
    "subset = subset.map(lambda x: {\"tokens\": tokenizer.encode(x[\"content\"]).ids})\n",
    "print(\"after tokenize dataset size: \", subset.shape)\n",
    "print()\n",
    "print(subset[\"tokens\"][0])  # Example tokenized output\n",
    "print()\n",
    "#\n",
    "# # VIEW SOME TOKENS:\n",
    "# # show each token for first 10:\n",
    "# for i in range(100):\n",
    "#     print(subset_tokenizd[\"tokens\"][0][i], tokenizer.decode([subset_tokenizd[\"tokens\"][0][i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(tokens, seq_len):\n",
    "    # Split into sequences of length seq_len + 1\n",
    "    sequences = [tokens[i:i + seq_len + 1] for i in range(len(tokens) - seq_len)]\n",
    "    return sequences\n",
    "\n",
    "seq_len = 50\n",
    "subset = subset.map(lambda x: {\"sequences\": create_training_pairs(x[\"tokens\"], seq_len)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECTING SUBSET with new columns:\n",
    "def print_subset():\n",
    "    print(subset.shape)\n",
    "    for name in subset.column_names:\n",
    "        print(f\"{name}: {subset[name]}\")\n",
    "        print(f\"  {len(subset[name])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_subset_new_columns():\n",
    "    for name in subset.column_names:\n",
    "        if name in original_columns:\n",
    "            continue\n",
    "        print(f\"{name}: {subset[name]}\")\n",
    "        first_record = subset[name][0]\n",
    "        if isinstance(first_record, list):\n",
    "            print(f\"  {first_record}\")\n",
    "            print(f\"  {len(first_record)} records\")\n",
    "        # if list has lists:\n",
    "        first_element_of_first_record = first_record[0]\n",
    "        if isinstance(first_element_of_first_record, list):\n",
    "            print(f\"    {first_element_of_first_record}\")\n",
    "            print(f\"    {len(first_element_of_first_record)} items\")\n",
    "        \n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "print_subset()\n",
    "\n",
    "print()\n",
    "print(\"# SEQUENCES:\")\n",
    "print()\n",
    "for i in range(3):\n",
    "    # each record (in original dataset) now has a list of sequences (so, sequences is not a scalar value like content)\n",
    "    print(\"## sequence \" + str(i))\n",
    "    print(subset[\"sequences\"][0][i])\n",
    "    print(tokenizer.decode(subset[\"sequences\"][0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input and target\n",
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "# wrong:\n",
    "# def mapper(x):\n",
    "#     what = x['sequences']\n",
    "#     split = split_input_target(what)\n",
    "#     print(f\"* {len(what)} -> {len(split[0])} {len(split[1])}\")\n",
    "#     return {\"x_input\": split[0], \"y_target\": split[1]}\n",
    "\n",
    "def mapper(all):\n",
    "    sequences = all[\"sequences\"]\n",
    "    splits = [split_input_target(sequence) for sequence in sequences]\n",
    "    x_input = [split[0] for split in splits]\n",
    "    y_target =  [split[1] for split in splits]\n",
    "    return {\"x_input\": x_input, \"y_target\": y_target}\n",
    "subset = subset.map(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_subset_new_columns()\n",
    "a = [1,2,3,4]\n",
    "print(a[:-1])\n",
    "print(a[1:])\n",
    "print(split_input_target(a))\n",
    "\n",
    "print_subset_new_columns()\n",
    "\n",
    "print(subset[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CodeCompletionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer(x, x)  # Encoder-only transformer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# mps_device = torch.device(\"mps\")\n",
    "model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=256, nhead=4, num_layers=4, seq_len=seq_len)\n",
    "model = model.to(mps_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ALTERNATE (less complexity, should train faster and maybe be useful)\n",
    "class AlternateCodeCompletionLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "# model = AlternateCodeCompletionLSTM(vocab_size=vocab_size, embed_size=128, hidden_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Prepare data\n",
    "# inputs = subset[\"x_input\"]\n",
    "# targets = subset[\"y_target\"]\n",
    "#inputs = subset[\"x_input\"][0]\n",
    "#targets = subset[\"y_target\"][0]\n",
    "inputs = torch.tensor([item for sublist in subset[\"x_input\"] for item in sublist], device=mps_device)\n",
    "targets = torch.tensor([item for sublist in subset[\"y_target\"] for item in sublist], device=mps_device)\n",
    "train_loader = DataLoader(TensorDataset(inputs, targets), batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "LOG_PROGRESS_TO_FILE = True\n",
    "LOG_FILE = f\"{scenario_path}/train.log\"\n",
    "if LOG_PROGRESS_TO_FILE:\n",
    "    # append, dont ever overwrite logs (del by hand if needed)\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        start_message = f\"\\n\\n## training starting: {time.asctime()}\\n\"\n",
    "        f.write(start_message)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "# Training loop\n",
    "for epoch in range(100):  # Adjust epochs as needed\n",
    "    total_batches = 0\n",
    "    start_epoch_time = time.perf_counter()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_batches += 1\n",
    "        if total_batches % 100 == 0:\n",
    "            sec_per_batch = (time.perf_counter() - start_epoch_time) / total_batches\n",
    "            epoch_sec_remaining = (len(train_loader) - total_batches) * sec_per_batch\n",
    "            epoch_remain_time = time.strftime(\"%H:%M:%S\", time.gmtime(epoch_sec_remaining))\n",
    "            epoch_elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.perf_counter() - start_epoch_time))\n",
    "            elapsed_seconds = time.perf_counter() - start_time  # Elapsed time\n",
    "            elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_seconds))\n",
    "            avg_epoch_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_seconds / (epoch + 1)))\n",
    "\n",
    "            log = f\"\\rbatch: {total_batches}/{len(train_loader)}, \" \\\n",
    "                f\"epoch_elapsed_time: {epoch_elapsed_time}, \" \\\n",
    "                f\"epoch_remain_time: {epoch_remain_time}, \" \\\n",
    "                f\"overall_time: {elapsed_time}, \" \\\n",
    "                f\"epoch: {epoch}, avg_epoch_time: {avg_epoch_time}, \" \\\n",
    "                f\"loss: {loss.item():.4f}\"\n",
    "            sys.stdout.write(log)\n",
    "            sys.stdout.flush()\n",
    "            if LOG_PROGRESS_TO_FILE:\n",
    "                with open(LOG_FILE, \"a\") as f:\n",
    "                    f.write(log + \"\\n\")\n",
    "# 40 epochs on first record sequences only... got to 11/17% loss, IIAC because data set is small, its easy to overfit it?\n",
    "# 50 epochs => 0.029 down to 0.005 loss... and it gives reasonable completions for the one example, well formatted shell script code\n",
    "# 2m => 30 epochs (20 to 50)... => MPS => 20 epochs in 32s.... ~45s for 30 (big gains vs 2m CPU!)\n",
    "# 10 records => IIGC 6 minutes for 20 epochs, right now epoch 6 is loss 0.416! 7 is 0.3491, 8 is .233, 9 is 0.2001, 10 is 0.186, 35 0.043, 36 0.009,37 0.359,38 0.261, 39 0.0261, 40 0.026522\n",
    "#\n",
    "# full corpus + 1 record => loss fell off rapidly but struggled to get as low as fast (0.10 ish stuck after 50ish epochs, down to 0.08 after 80 epochs IIRC)\n",
    "#    full corups definitely has smaller tokens than one/ten records, to be expected IIAC\n",
    "#    full w/ 10 records => faster than 10 records as corpus... 9s per epoch this time (was 15ish before)... 10 0.288 loss, 11 0.20777, 19 0.0671, 20 0.11686!, 21 0.128, 22 0.041, 23 0.144, 24 0.074, 25 0.169, 26 0.016, 27 0.1099, 28 0.048, 29 0.0457, 32 0.0321, 40 0.0319 (probably about as good as can be expected for model and limited data)\n",
    "#       * much better responses when starting in the middle of an if block (past start of if)... I get reasonable syntax after wards and goodish suggestions... things are working better in this case with more data and bigger corpus it seems\n",
    "#\n",
    "# 100 records test => estimate 3m20s-#1(epoch),6m43s-#2 per epoch\n",
    "#    => 100 epochs == 5.6+ hours...\n",
    "#       not even done by morning :)...\n",
    "#    just wanna know does errro do better than ~3 (epoch 1 is 4.38 so that is promising, 2 is 3.8738)\n",
    "#       with my padded model I can't get error to drop below 3 after 100+ epochs too w/ 100 records\n",
    "#    ok yeah after 5 it is still in the 3.5+ range... seems stuck there out of the gate...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{scenario_path}/checkpoint003-100records-100?epochs-loss?.pth\")\n",
    "\n",
    "def load_model():\n",
    "    model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "    # Load the saved state_dict into the model\n",
    "    model.load_state_dict(torch.load(f\"{scenario_path}/checkpoint001.pth\"))\n",
    "    #model.to(mps_device)\n",
    "\n",
    "# load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(model, tokenizer, prompt, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).ids  # Convert prompt to tokens\n",
    "    if len(tokens) < max_len:\n",
    "        raise Exception(\"prompt too short\")\n",
    "        # pad didn't \"work\" but I only tested it one time so yeah... and on a miniscule dataset :)\n",
    "        padding = tokenizer.encode(\"\\n\" * (max_len - len(tokens))).ids\n",
    "        padding.extend(tokens)\n",
    "        tokens = padding\n",
    "\n",
    "    print(\"tokens:\", tokens)\n",
    "    generated_tokens = []\n",
    "    tokens = tokens[-max_len:]  # truncate to max_len\n",
    "    original_decoded = tokenizer.decode(tokens)\n",
    "    for _ in range(max_len):\n",
    "        last_max_len_tokens = tokens[-max_len:]\n",
    "        input_tensor = torch.tensor(last_max_len_tokens).unsqueeze(0).to(mps_device)  # Add batch dimension\n",
    "        output = model(input_tensor)  #.contiguous().to(mps_device))  # Predict next token\n",
    "        next_token = output[0, -1].argmax(-1).item()  # Get the highest probability token\n",
    "        tokens.append(next_token)  # Add the next token\n",
    "        generated_tokens.append(next_token)\n",
    "        if next_token == tokenizer.token_to_id(\"<END>\"):  # Stop at <END> token\n",
    "            break\n",
    "    return original_decoded, tokenizer.decode(generated_tokens)\n",
    "\n",
    "# *** ONE RECORD prompts with ONE RECORD AS CORPUS *** (note does not work with 10 records using 10 records as corpus b/c diff tokens..)\n",
    "# TODO padding issue, NEED TO FIX in tokenizer? and? retrain? or? just pad bogus chars?\n",
    "# print(generate_code(model, tokenizer, \"#!/bin/bash\\n\"))\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\nfi\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\techo 1\\n\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuckfaceretarted\"  # too short\n",
    "# wow in the case of me rambling swear words, it put more gibberish after it like random words too! cool... versus an `fi` to close out\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuck'\\n\"  # HERE b/c it was closed it started to add diff logic below.. wrong but interesting to generate what was complete logic blocks (partially correct just not across mutliple lines but who cares... it even got indentation and changed out what was happening in an if block!)\n",
    "# test_prompt = \"KUBECTL='docker exec hyperkube /hyperkube kubectl'\\n\\n#RUN_SKYDNS=\\\"yes\\\"\\nRUN_SKYDNS=\\\"no\\\"\\n# DNS_ARGUMENTS needs to be p\"\n",
    "\n",
    "\n",
    "# *** 10 records prompts (with 10 records as corpus) ***\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\nfi\\n\"\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\n\"\n",
    "# FYI not as logical as 1 record output... b/c its not memorizing most likely and not yet learning \n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "original, generated = generate_code(model, tokenizer, test_prompt)\n",
    "print(\"## all:\")\n",
    "print(f\"{Fore.GREEN}{original}{Style.RESET_ALL}{Fore.RED}{generated}{Style.RESET_ALL}\")\n",
    "print()\n",
    "# TODO strip new lines? or can I collapse down to one with a count?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
