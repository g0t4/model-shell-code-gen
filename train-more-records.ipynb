{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    # FYI export HF_TOKEN=$(pbpaste)\n",
    "    raise ValueError(\"You need to set HF_TOKEN environment variable. Get one from huggingface.co\")\n",
    "\n",
    "# Load the shell/bash subset\n",
    "#   FYI:   https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/shell (FYI gated, must give email and approve)\n",
    "full_dataset = load_dataset(\"bigcode/the-stack\", split=\"train\", data_dir=\"data/shell\")  # , lang=[\"bash\"])\n",
    "# data_dir data/shell has 11 files, not bad size (about 4GB IIEC)\n",
    "\n",
    "# Save locally in diff format if needed, I am going to stick with parquet\n",
    "# full_dataset.to_csv(\"shell_scripts.csv\")\n",
    "# print(full_dataset.column_names)\n",
    "num_records = 100\n",
    "subset = full_dataset.take(num_records)\n",
    "print(\"shape\", subset.shape)\n",
    "print(f\"  records({len(subset)}): size(bytes) comma delimited: {subset.data.nbytes/1024/1024:,.4f} MB\")\n",
    "\n",
    "original_columns = subset.column_names\n",
    "\n",
    "scenario_path = f\"tmp/04-more-{num_records}\"\n",
    "# ensure scenario dir exists, but don't wipe it if it already exists, will wipe/replace selective subsets (i.e. I want model checkpoints preserved)\n",
    "os.makedirs(scenario_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace is fine b/c I need to change file size... forgot to do that with the 10 records model trains w00ps\n",
    "corpus_file = f\"{scenario_path}/corpus\"\n",
    "with open(corpus_file, \"w\") as f:\n",
    "    for example in subset[\"content\"]:  # Adjust \"content\" to match your dataset key\n",
    "        f.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# FYI not using the saved tokenizer (yet) ... but takes very little time to train and save so just do it\n",
    "# recreate tokenzier model\n",
    "tokenizer_path = f\"{scenario_path}/tokenizer\"\n",
    "if os.path.exists(tokenizer_path):\n",
    "    os.system(f\"trash {tokenizer_path}\")\n",
    "os.makedirs(tokenizer_path)\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[corpus_file], vocab_size=8000, min_frequency=2)  # PRN adjust vocab_size/min_frequency?\n",
    "tokenizer.save_model(tokenizer_path)\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(tokenizer_path + \"/vocab.json\", tokenizer_path + \"/merges.txt\")\n",
    "\n",
    "subset = subset.map(lambda x: {\"tokens\": tokenizer.encode(x[\"content\"]).ids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(tokens, seq_len):\n",
    "    # Split into sequences of length seq_len + 1\n",
    "    sequences = [tokens[i:i + seq_len + 1] for i in range(len(tokens) - seq_len)]\n",
    "    return sequences\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "subset = subset.map(lambda x: {\"sequences\": create_training_pairs(x[\"tokens\"], seq_len)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "\n",
    "def mapper(all):\n",
    "    sequences = all[\"sequences\"]\n",
    "    splits = [split_input_target(sequence) for sequence in sequences]\n",
    "    x_input = [split[0] for split in splits]\n",
    "    y_target = [split[1] for split in splits]\n",
    "    return {\"x_input\": x_input, \"y_target\": y_target}\n",
    "\n",
    "\n",
    "subset = subset.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CodeCompletionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer(x, x)  # Encoder-only transformer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "vocab_size = 8000\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def initialize_model() -> CodeCompletionTransformer:\n",
    "    model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "    model = model.to(mps_device)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ALTERNATE (less complexity, should train faster and maybe be useful)\n",
    "class AlternateCodeCompletionLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "# model = AlternateCodeCompletionLSTM(vocab_size=8000, embed_size=128, hidden_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "class CodeDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "\n",
    "# move to mps before training loop even starts (should reduce small overhead in calling .to(mps_device) even on a mac with unified memory, IIUC)\n",
    "inputs = torch.tensor([item for sublist in subset[\"x_input\"] for item in sublist]).to(mps_device)\n",
    "targets = torch.tensor([item for sublist in subset[\"y_target\"] for item in sublist]).to(mps_device)\n",
    "\n",
    "train_loader = DataLoader(CodeDataset(inputs, targets), batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):  # Adjust epochs as needed\n",
    "    for inputs, targets in train_loader:\n",
    "        # pause if pause file is present? or have a stop and resume mechanism\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Iteration {epoch} loss: {loss.item()}\")\n",
    "    # sleep(30) # with mps, cpu is staying at 115% which (instead of 700%) so I am less worried about the fan now\n",
    "\n",
    "# # wow loss on 10 records and 10 epocs only... is already almost as low as 1 record for 40 epochs...\n",
    "# # 15m per 10 epochs with 10 records... so let's do 100 epochs while I sleep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model:\n",
    "checkpoint_dir = f\"{scenario_path}/checkpoints/\"\n",
    "checkpoint_path = f\"{checkpoint_dir}/01-X.pth\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    new_model = initialize_model()\n",
    "    # Load the saved state_dict into the model\n",
    "    new_model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return new_model\n",
    "\n",
    "\n",
    "# model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** EVALUATE\n",
    "\n",
    "\n",
    "def generate_code(model, tokenizer, prompt, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).ids  # Convert prompt to tokens\n",
    "    if len(tokens) < max_len:\n",
    "        raise Exception(\"prompt too short\")\n",
    "        # pad didn't \"work\" but I only tested it one time so yeah... and on a miniscule dataset :)\n",
    "        padding = tokenizer.encode(\"\\n\" * (max_len - len(tokens))).ids\n",
    "        padding.extend(tokens)\n",
    "        tokens = padding\n",
    "\n",
    "    print(\"tokens:\", tokens)\n",
    "    generated_tokens = []\n",
    "    original_tokens = tokens.copy()\n",
    "    for _ in range(max_len):\n",
    "        last_max_len_tokens = tokens[-max_len:]\n",
    "        # TODO disable gradients while evaluating?\n",
    "        input_tensor = torch.tensor(last_max_len_tokens).unsqueeze(0).to(mps_device)  # Add batch dimension\n",
    "        output = model(input_tensor)  # Predict next token\n",
    "        next_token = output[0, -1].argmax(-1).item()  # Get the highest probability token\n",
    "        tokens.append(next_token)  # Add the next token\n",
    "        generated_tokens.append(next_token)\n",
    "        if next_token == tokenizer.token_to_id(\"<END>\"):  # Stop at <END> token\n",
    "            break\n",
    "    return tokenizer.decode(original_tokens), tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "# TODO padding issue, NEED TO FIX in tokenizer? and? retrain? or? just pad bogus chars?\n",
    "# print(generate_code(model, tokenizer, \"#!/bin/bash\\n\"))\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\nfi\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\techo 1\\n\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuckfaceretarted\" # too short\n",
    "# wow in the case of me rambling swear words, it put more gibberish after it like random words too! cool... versus an `fi` to close out\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuck'\\n\"  # HERE b/c it was closed it started to add diff logic below.. wrong but interesting to generate what was complete logic blocks (partially correct just not across mutliple lines but who cares... it even got indentation and changed out what was happening in an if block!)\n",
    "# test_prompt = \"KUBECTL='docker exec hyperkube /hyperkube kubectl'\\n\\n#RUN_SKYDNS=\\\"yes\\\"\\nRUN_SKYDNS=\\\"no\\\"\\n# DNS_ARGUMENTS needs to be pa\"\n",
    "# if [ \"${RUN_SKYDNS}\" = \"yes\" ]; then\n",
    "# \tDNS_ARGUMENTS=\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\"\n",
    "# else\n",
    "# \tDNS_ARGUMENTS=\"\"\n",
    "# fi\n",
    "# \"\n",
    "from colorama import Fore, Style\n",
    "\n",
    "original, generated = generate_code(model, tokenizer, test_prompt)\n",
    "print(\"## all:\")\n",
    "print(f\"{Fore.GREEN}{original}{Style.RESET_ALL}{Fore.RED}{generated}{Style.RESET_ALL}\")\n",
    "print()\n",
    "# TODO strip new lines? or can I collapse down to one with a count?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
