{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    # FYI export HF_TOKEN=$(pbpaste)\n",
    "    raise ValueError(\"You need to set HF_TOKEN environment variable. Get one from huggingface.co\")\n",
    "\n",
    "# Load the shell/bash subset\n",
    "#   FYI:   https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/shell (FYI gated, must give email and approve)\n",
    "full_dataset = load_dataset(\"bigcode/the-stack\", split=\"train\", data_dir=\"data/shell\")  # , lang=[\"bash\"])\n",
    "print(f\"  records({len(full_dataset)}): size(bytes) comma delimited: {full_dataset.data.nbytes/1024/1024:,.4f} MB\")\n",
    "# data_dir data/shell has 11 files, not bad size (about 4GB IIEC)\n",
    "\n",
    "# Save locally in diff format if needed, I am going to stick with parquet\n",
    "# full_dataset.to_csv(\"shell_scripts.csv\")\n",
    "# print(full_dataset.column_names)\n",
    "num_records = 100\n",
    "seq_len = 50  # !!! TODO figure out timing for 1024 vs 256\n",
    "subset = full_dataset.take(num_records)\n",
    "print(f\"subset: shape({subset.shape}) len({len(subset)}) type({type(subset)})\")\n",
    "print(f\"  records({len(subset)}): size(bytes) comma delimited: {subset.data.nbytes/1024/1024:,.4f} MB\")\n",
    "\n",
    "scenario_path = f\"tmp/more/05-records{num_records}-seqlen{seq_len}\"\n",
    "# ensure scenario dir exists, but don't wipe it if it already exists, will wipe/replace selective subsets (i.e. I want model checkpoints preserved)\n",
    "os.makedirs(scenario_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace is fine b/c I need to change file size... forgot to do that with the 10 records model trains w00ps\n",
    "corpus_file = f\"{scenario_path}/corpus\"\n",
    "if not os.path.exists(corpus_file):\n",
    "    with open(corpus_file, \"w\") as f:\n",
    "        for example in subset[\"content\"]:  # Adjust \"content\" to match your dataset key\n",
    "            f.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# FYI not using the saved tokenizer (yet) ... but takes very little time to train and save so just do it\n",
    "# recreate tokenzier model\n",
    "tokenizer_path = f\"{scenario_path}/tokenizer\"\n",
    "os.makedirs(tokenizer_path, exist_ok=True)\n",
    "if os.path.exists(tokenizer_path + \"/vocab.json\"):\n",
    "    raise ValueError(f\"tokenizer_path {tokenizer_path} already exists, not recreating... you need to reload it\")\n",
    "\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[corpus_file], vocab_size=8000, min_frequency=2)  # PRN adjust vocab_size/min_frequency?\n",
    "tokenizer.save_model(tokenizer_path)\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "def load_saved_tokenizer():\n",
    "    return ByteLevelBPETokenizer(tokenizer_path + \"/vocab.json\", tokenizer_path + \"/merges.txt\")\n",
    "\n",
    "\n",
    "tokenizer = load_saved_tokenizer()\n",
    "\n",
    "subset = subset.map(lambda x: {\"tokens\": tokenizer.encode(x[\"content\"]).ids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(tokens, seq_len):\n",
    "    # Split into sequences of length seq_len + 1\n",
    "    # ahh crap the +1 here made for sequences of seq_len+1 (i.e. 512 => 513) so that when x_input/y_target split happens, its 512 in/out with model and not 511\n",
    "    sequences = [tokens[i:i + seq_len + 1] for i in range(len(tokens) - seq_len)]\n",
    "    return sequences\n",
    "\n",
    "\n",
    "subset = subset.map(lambda x: {\"sequences\": create_training_pairs(x[\"tokens\"], seq_len)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "\n",
    "def mapper(all):\n",
    "    sequences = all[\"sequences\"]\n",
    "    splits = [split_input_target(sequence) for sequence in sequences]\n",
    "    x_input = [split[0] for split in splits]\n",
    "    y_target = [split[1] for split in splits]\n",
    "    return {\"x_input\": x_input, \"y_target\": y_target}\n",
    "\n",
    "\n",
    "subset = subset.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = list(subset[\"x_input\"])\n",
    "y_target = list(subset[\"y_target\"])\n",
    "sequences = list(subset[\"sequences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sequences len: {len(sequences)}  type: {type(sequences)}\")\n",
    "print(f\"x_input len: {len(x_input)}  type: {type(x_input)}\")\n",
    "print(f\"y_target len: {len(y_target)}  type: {type(y_target)}\")\n",
    "\n",
    "for i in range(len(sequences)):\n",
    "    if len(sequences[i]) == 0:\n",
    "        continue\n",
    "    print(f\"sequences[{i}] len: {len(sequences[i])}  type: {type(sequences[i])}\")\n",
    "    print(f\"  sequences[{i}][0] len: {len(sequences[i][0])} type: {type(sequences[i][0])}\")\n",
    "    print(f\"x_input[{i}] len: {len(x_input[i])}  type: {type(x_input[i])}\")\n",
    "    print(f\"  x_input[{i}][0] len: {len(x_input[i][0])} type: {type(x_input[i][0])}\")\n",
    "    print(f\"y_target[{i}] len: {len(y_target[i])}  type: {type(y_target[i])}\")\n",
    "    print(f\"  y_target[{i}][0] len: {len(y_target[i][0])} type: {type(y_target[i][0])}\")\n",
    "\n",
    "    print(f\"sequences[{i}][0]: {sequences[i][0]}\")\n",
    "    print(f\"x_input[{i}][0]: {x_input[i][0]}\")\n",
    "    print(f\"y_target[{i}][0]: {y_target[i][0]}\")\n",
    "    break  # stop on first match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CodeCompletionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer(x, x)  # Encoder-only transformer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "vocab_size = 8000\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def initialize_model() -> CodeCompletionTransformer:\n",
    "    model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "    model = model.to(mps_device)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ALTERNATE (less complexity, should train faster and maybe be useful)\n",
    "class AlternateCodeCompletionLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "# model = AlternateCodeCompletionLSTM(vocab_size=8000, embed_size=128, hidden_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "# build the train_loader\n",
    "\n",
    "# FYI should already be on mps device b/c I am loading into a tensor with dataset load above\n",
    "inputs = [item for sublist in subset[\"x_input\"] for item in sublist]\n",
    "# print(f\"inputs: {len(inputs)} {type(inputs)} - {inputs[0][0]} {type(inputs[0][0])}\")\n",
    "#  FYI can use int32 or smaller for weights of inputs/targets but shouldn't matter much so leave int64 default for now\n",
    "targets = [item for sublist in subset[\"y_target\"] for item in sublist]\n",
    "inputs_tensor = torch.tensor(inputs, device=mps_device)\n",
    "targets_tensor = torch.tensor(targets, device=mps_device)\n",
    "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "print(f\"inputs tensor: {inputs_tensor.shape} {inputs_tensor.dtype}\")\n",
    "print(f\"targets tensor: {targets_tensor.shape} {targets_tensor.dtype}\")\n",
    "print(f\"train_loader: {len(train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "start_time = time.perf_counter()\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    for epoch in range(num_epochs):  # Adjust epochs as needed\n",
    "        total_batches = 0\n",
    "        start_epoch_time = time.perf_counter()\n",
    "        for inputs, targets in train_loader:\n",
    "            # ! TODO consider profiling for optimizations? TorchScript Profiler, Autograd Profiler, torch.utils.bottleneck, torch.profiler ...?\n",
    "            # pause if pause file is present? or have a stop and resume mechanism\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_batches += 1\n",
    "            if total_batches % 10 == 0:\n",
    "                sec_per_batch = (time.perf_counter() - start_epoch_time) / total_batches\n",
    "                epoch_sec_remaining = (len(train_loader) - total_batches) * sec_per_batch\n",
    "                epoch_remain_time = time.strftime(\"%H:%M:%S\", time.gmtime(epoch_sec_remaining))\n",
    "                epoch_elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.perf_counter() - start_epoch_time))\n",
    "                overall_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.perf_counter() - start_time))\n",
    "                sys.stdout.write(f\"\\repoch: {epoch+1}, \"\n",
    "                                 f\"batch: {total_batches}/{len(train_loader)}, \"\n",
    "                                 f\"epoch_elapsed_time: {epoch_elapsed_time}, \"\n",
    "                                 f\"epoch_remain_time: {epoch_remain_time}, \"\n",
    "                                 f\"overall_time: {overall_time},\"\n",
    "                                 f\"loss: {loss.item():.4f}\")\n",
    "                sys.stdout.flush()\n",
    "            # 1024 seq => 22m/epoch (<2s/batch * 849batches/epoch)\n",
    "            # 512 seq => 11m19s/epoch\n",
    "            # 256 seq => 6m/epoch (1577 batches/epoch)\n",
    "            # 50 seq => 3m/epoch\n",
    "\n",
    "        # FYI indent this to tmp test the output and reverse indent so it only runs per epoch otherwise\n",
    "        # FYI 1-3ms per call to print progress (depending on complexity of calculations)\n",
    "        elapsed_seconds = time.perf_counter() - start_time  # Elapsed time\n",
    "        elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_seconds))\n",
    "        avg_epoch_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_seconds / (epoch + 1)))\n",
    "        # each write to stdout < 1ms\n",
    "        sys.stdout.flush()\n",
    "#! TODO LET THIS ONE RUN FOR A WHILE AND GET THE LOSS CURVE OVER TIME so I have it as something of a ref... I recall getting really low loss quickly on 1 record but I cant recall what it was for 100\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model:\n",
    "checkpoint_dir = f\"{scenario_path}/checkpoints/\"\n",
    "checkpoint_path = f\"{checkpoint_dir}/01-epoch11-2h11m.pt\"\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "# torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # TODO MAKE SURE TO RELOAD THE TOKENIZER!! OMG dont re-create it!\n",
    "    new_tokenizer = load_saved_tokenizer()\n",
    "    new_model = initialize_model()\n",
    "    # Load the saved state_dict into the model\n",
    "    new_model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return new_tokenizer, new_model\n",
    "\n",
    "\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** EVALUATE\n",
    "\n",
    "\n",
    "def generate_code(model, tokenizer, prompt, max_len=seq_len):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).ids  # Convert prompt to tokens\n",
    "    if len(tokens) < max_len:\n",
    "        raise Exception(\"prompt too short\")\n",
    "        # pad didn't \"work\" but I only tested it one time so yeah... and on a miniscule dataset :)\n",
    "        padding = tokenizer.encode(\"\\n\" * (max_len - len(tokens))).ids\n",
    "        padding.extend(tokens)\n",
    "        tokens = padding\n",
    "\n",
    "    print(\"tokens:\", tokens)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[-max_len:]\n",
    "    generated_tokens = []\n",
    "    original_tokens = tokens.copy()\n",
    "    for _ in range(max_len):\n",
    "        last_max_len_tokens = tokens[-max_len:]\n",
    "        # TODO disable gradients while evaluating?\n",
    "        input_tensor = torch.tensor(last_max_len_tokens).unsqueeze(0).to(mps_device)  # Add batch dimension\n",
    "        output = model(input_tensor)  # Predict next token\n",
    "        next_token = output[0, -1].argmax(-1).item()  # Get the highest probability token\n",
    "        tokens.append(next_token)  # Add the next token\n",
    "        generated_tokens.append(next_token)\n",
    "        if next_token == tokenizer.token_to_id(\"<END>\"):  # Stop at <END> token\n",
    "            break\n",
    "    return tokenizer.decode(original_tokens), tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "# TODO padding issue, NEED TO FIX in tokenizer? and? retrain? or? just pad bogus chars?\n",
    "# print(generate_code(model, tokenizer, \"#!/bin/bash\\n\"))\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\n\\n\"\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\"\n",
    "test_prompt = \"if [ \\\"${IS_MACOS}\\\" = \\\"yes\\\" ]; then\\n  echo \\\"this is a mac, details: $(uname)\\\"\\nelseif [ \\\"${IS_LINUX}\\\" = \\\"yes\\\" ]; then\\n  echo \\\"this is linux\\\"\\nfi\"\n",
    "\n",
    "# THIS DOES AWESOME on seq_len=50 (scenario 04)... \n",
    "test_prompt = \"if [ \\\"${IS_MACOS}\\\" = \\\"yes\\\" ]; then\\n  echo \\\"mac\\\"\\nelseif [ \\\"${IS_LINUX}\\\" = \\\"yes\\\" ]; then\\n  echo \\\"linux\\\"\\nelse\\n  \"\n",
    "# I am excited to figure out how to pad tokens in eval (see chatgpt thread) s/b to basically ignore the pad tokens in the model and add that after training should work fine\n",
    "\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuckfaceretarted\" # too short\n",
    "# wow in the case of me rambling swear words, it put more gibberish after it like random words too! cool... versus an `fi` to close out\n",
    "# test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuck'\\n\"  # HERE b/c it was closed it started to add diff logic below.. wrong but interesting to generate what was complete logic blocks (partially correct just not across mutliple lines but who cares... it even got indentation and changed out what was happening in an if block!)\n",
    "# test_prompt = \"KUBECTL='docker exec hyperkube /hyperkube kubectl'\\n\\n#RUN_SKYDNS=\\\"yes\\\"\\nRUN_SKYDNS=\\\"no\\\"\\n# DNS_ARGUMENTS needs to be pa\"\n",
    "# if [ \"${RUN_SKYDNS}\" = \"yes\" ]; then\n",
    "# \tDNS_ARGUMENTS=\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\"\n",
    "# else\n",
    "# \tDNS_ARGUMENTS=\"\"\n",
    "# fi\n",
    "# \"\n",
    "from colorama import Fore, Style\n",
    "\n",
    "original, generated = generate_code(model, tokenizer, test_prompt)\n",
    "print(\"## all:\")\n",
    "print(f\"{Fore.GREEN}{original}{Style.RESET_ALL}{Fore.RED}{generated}{Style.RESET_ALL}\")\n",
    "print()\n",
    "# TODO strip new lines? or can I collapse down to one with a count?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
