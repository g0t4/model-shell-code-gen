{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    # FYI export HF_TOKEN=$(pbpaste)\n",
    "    raise ValueError(\"You need to set HF_TOKEN environment variable. Get one from huggingface.co\")\n",
    "\n",
    "# Load the shell/bash subset\n",
    "#   FYI:   https://huggingface.co/datasets/bigcode/the-stack/tree/main/data/shell (FYI gated, must give email and approve)\n",
    "full_dataset = load_dataset( \"bigcode/the-stack\", split=\"train\", data_dir=\"data/shell\")  # , lang=[\"bash\"])\n",
    "# data_dir data/shell has 11 files, not bad size (about 4GB IIEC)\n",
    "\n",
    "# Save locally in diff format if needed, I am going to stick with parquet\n",
    "# full_dataset.to_csv(\"shell_scripts.csv\")\n",
    "# print(full_dataset.column_names)\n",
    "subset = full_dataset.select(range(10))\n",
    "print(\"shape\", subset.shape)\n",
    "\n",
    "original_columns = subset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus\n",
    "if not os.path.exists(\"tmp\"):\n",
    "    os.makedirs(\"tmp\")\n",
    "\n",
    "corpus_file = \"tmp/shell_scripts_corpus.sh\"\n",
    "if not os.path.exists(corpus_file):\n",
    "    with open(corpus_file, \"w\") as f:\n",
    "        for example in subset[\"content\"]:  # Adjust \"content\" to match your dataset key\n",
    "            f.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer_path = \"tmp/trained-tokenizer\"\n",
    "if not os.path.exists(tokenizer_path):\n",
    "    os.makedirs(tokenizer_path)\n",
    "\n",
    "# Train the tokenizer\n",
    "if not os.path.exists(tokenizer_path + \"/vocab.json\"):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[corpus_file], vocab_size=8000, min_frequency=2)  # PRN adjust vocab_size/min_frequency?\n",
    "    tokenizer.save_model(tokenizer_path)\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(tokenizer_path + \"/vocab.json\", tokenizer_path + \"/merges.txt\")\n",
    "\n",
    "# tokenize (FYI .map is immutable, a new dataset is returned with the added column)\n",
    "print(\"before tokenize dataset size: \", subset.shape)\n",
    "subset = subset.map(lambda x: {\"tokens\": tokenizer.encode(x[\"content\"]).ids})\n",
    "print(\"after tokenize dataset size: \", subset.shape)\n",
    "print()\n",
    "print(subset[\"tokens\"][0])  # Example tokenized output\n",
    "print()\n",
    "#\n",
    "# # VIEW SOME TOKENS:\n",
    "# # show each token for first 10:\n",
    "# for i in range(100):\n",
    "#     print(subset_tokenizd[\"tokens\"][0][i], tokenizer.decode([subset_tokenizd[\"tokens\"][0][i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(tokens, seq_len):\n",
    "    # Split into sequences of length seq_len + 1\n",
    "    sequences = [tokens[i:i + seq_len + 1] for i in range(len(tokens) - seq_len)]\n",
    "    return sequences\n",
    "\n",
    "seq_len = 50\n",
    "subset = subset.map(lambda x: {\"sequences\": create_training_pairs(x[\"tokens\"], seq_len)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECTING SUBSET with new columns:\n",
    "def print_subset():\n",
    "    print(subset.shape)\n",
    "    for name in subset.column_names:\n",
    "        print(f\"{name}: {subset[name]}\")\n",
    "        print(f\"  {len(subset[name])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_subset_new_columns():\n",
    "    for name in subset.column_names:\n",
    "        if name in original_columns:\n",
    "            continue\n",
    "        print(f\"{name}: {subset[name]}\")\n",
    "        first_record = subset[name][0]\n",
    "        if isinstance(first_record, list):\n",
    "            print(f\"  {first_record}\")\n",
    "            print(f\"  {len(first_record)} records\")\n",
    "        # if list has lists:\n",
    "        first_element_of_first_record = first_record[0]\n",
    "        if isinstance(first_element_of_first_record, list):\n",
    "            print(f\"    {first_element_of_first_record}\")\n",
    "            print(f\"    {len(first_element_of_first_record)} items\")\n",
    "        \n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "print_subset()\n",
    "\n",
    "print()\n",
    "print(\"# SEQUENCES:\")\n",
    "print()\n",
    "for i in range(3):\n",
    "    # each record (in original dataset) now has a list of sequences (so, sequences is not a scalar value like content)\n",
    "    print(\"## sequence \" + str(i))\n",
    "    print(subset[\"sequences\"][0][i])\n",
    "    print(tokenizer.decode(subset[\"sequences\"][0][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input and target\n",
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "# wrong:\n",
    "# def mapper(x):\n",
    "#     what = x['sequences']\n",
    "#     split = split_input_target(what)\n",
    "#     print(f\"* {len(what)} -> {len(split[0])} {len(split[1])}\")\n",
    "#     return {\"x_input\": split[0], \"y_target\": split[1]}\n",
    "\n",
    "def mapper(all):\n",
    "    sequences = all[\"sequences\"]\n",
    "    splits = [split_input_target(sequence) for sequence in sequences]\n",
    "    x_input = [split[0] for split in splits]\n",
    "    y_target =  [split[1] for split in splits]\n",
    "    return {\"x_input\": x_input, \"y_target\": y_target}\n",
    "subset = subset.map(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_subset_new_columns()\n",
    "a = [1,2,3,4]\n",
    "print(a[:-1])\n",
    "print(a[1:])\n",
    "print(split_input_target(a))\n",
    "\n",
    "print_subset_new_columns()\n",
    "\n",
    "print(subset[\"content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CodeCompletionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer(x, x)  # Encoder-only transformer\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "vocab_size = 8000\n",
    "model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "mps_device = torch.device(\"mps\")\n",
    "model = model.to(mps_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** ALTERNATE (less complexity, should train faster and maybe be useful)\n",
    "class AlternateCodeCompletionLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "# model = AlternateCodeCompletionLSTM(vocab_size=8000, embed_size=128, hidden_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DataLoader\n",
    "class CodeDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "# Prepare data\n",
    "# inputs = subset[\"x_input\"]\n",
    "# targets = subset[\"y_target\"]\n",
    "#inputs = subset[\"x_input\"][0]\n",
    "#targets = subset[\"y_target\"][0]\n",
    "inputs = [ item for sublist in subset[\"x_input\"] for item in sublist]\n",
    "targets = [ item for sublist in subset[\"y_target\"] for item in sublist]\n",
    "\n",
    "train_loader = DataLoader(CodeDataset(inputs, targets), batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "# TODO get this to run with mps optimziations, would it matter? Also review how I am doing any testing steps for inefficiencies\n",
    "for epoch in range(1000):  # Adjust epochs as needed\n",
    "    for subepoch in range(5):  # Adjust subepochs as needed\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(mps_device)\n",
    "            targets = targets.to(mps_device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch} {subepoch} loss: {loss.item()}\")\n",
    "    # sleep(30) # with mps, cpu is staying at 115% which (instead of 700%) so I am less worried about the fan now\n",
    "\n",
    "# # wow loss on 10 records and 10 epocs only... is already almost as low as 1 record for 40 epochs...\n",
    "# # 15m per 10 epochs with 10 records... so let's do 100 epochs while I sleep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model:\n",
    "torch.save(model.state_dict(), \"tmp/model-02-more10records-6epochs-plusX.pth\")\n",
    "\n",
    "#!!! TODO MPS?!!!\n",
    "\n",
    "def load_model():\n",
    "    model = CodeCompletionTransformer(vocab_size=vocab_size, d_model=128, nhead=4, num_layers=2, seq_len=seq_len)\n",
    "    # Load the saved state_dict into the model\n",
    "    model.load_state_dict(torch.load(\"tmp/model-40ish-epocs-first-record-only.pth\"))\n",
    "\n",
    "# load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** EVALUATE\n",
    "\n",
    "def generate_code(model, tokenizer, prompt, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).ids  # Convert prompt to tokens\n",
    "    if len(tokens) < max_len:\n",
    "        raise Exception(\"prompt too short\")\n",
    "        # pad didn't \"work\" but I only tested it one time so yeah... and on a miniscule dataset :)\n",
    "        padding = tokenizer.encode(\"\\n\"*(max_len-len(tokens))).ids\n",
    "        padding.extend(tokens)\n",
    "        tokens = padding\n",
    "\n",
    "    print(\"tokens:\", tokens)\n",
    "    generated_tokens = []\n",
    "    original_tokens = tokens.copy()\n",
    "    for _ in range(max_len):\n",
    "        last_max_len_tokens = tokens[-max_len:]\n",
    "        # TODO disable gradients while evaluating?\n",
    "        input_tensor = torch.tensor(last_max_len_tokens).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(input_tensor)  # Predict next token\n",
    "        next_token = output[0, -1].argmax(-1).item()  # Get the highest probability token\n",
    "        tokens.append(next_token)  # Add the next token\n",
    "        generated_tokens.append(next_token)\n",
    "        if next_token == tokenizer.token_to_id(\"<END>\"):  # Stop at <END> token\n",
    "            break\n",
    "    return tokenizer.decode(original_tokens), tokenizer.decode(generated_tokens)\n",
    "\n",
    "\n",
    "# TODO padding issue, NEED TO FIX in tokenizer? and? retrain? or? just pad bogus chars?\n",
    "# print(generate_code(model, tokenizer, \"#!/bin/bash\\n\"))\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\\nfi\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\tDNS_ARGUMENTS=\\\"\\\"\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n\tDNS_ARGUMENTS=\\\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\\\"\\nelse\\n\techo 1\\n\\n\"\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuckfaceretarted\" # too short\n",
    "# wow in the case of me rambling swear words, it put more gibberish after it like random words too! cool... versus an `fi` to close out\n",
    "test_prompt = \"if [ \\\"${RUN_SKYDNS}\\\" = \\\"yes\\\" ]; then\\n   echo 'suck a giant fucking dick you asslicking cutwaffle pumperkiffenslobistrapistfuck'\\n\"  # HERE b/c it was closed it started to add diff logic below.. wrong but interesting to generate what was complete logic blocks (partially correct just not across mutliple lines but who cares... it even got indentation and changed out what was happening in an if block!)\n",
    "# test_prompt = \"KUBECTL='docker exec hyperkube /hyperkube kubectl'\\n\\n#RUN_SKYDNS=\\\"yes\\\"\\nRUN_SKYDNS=\\\"no\\\"\\n# DNS_ARGUMENTS needs to be pa\"\n",
    "# if [ \"${RUN_SKYDNS}\" = \"yes\" ]; then\n",
    "# \tDNS_ARGUMENTS=\"--cluster-dns=10.0.0.10 --cluster-domain=cluster.local\"\n",
    "# else\n",
    "# \tDNS_ARGUMENTS=\"\"\n",
    "# fi\n",
    "# \"\n",
    "from colorama import Fore, Style\n",
    "\n",
    "original, generated = generate_code(model, tokenizer, test_prompt)\n",
    "print(\"## all:\")\n",
    "print(f\"{Fore.GREEN}{original}{Style.RESET_ALL}{Fore.RED}{generated}{Style.RESET_ALL}\")\n",
    "print()\n",
    "# TODO strip new lines? or can I collapse down to one with a count?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
